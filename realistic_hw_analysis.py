#!/usr/bin/env python3
"""
RealistickÃ¡ analÃ½za HW poÅ¾adavkÅ¯ pro bÄ›Å¾nÃ© servery a PC
"""

def realistic_hw_analysis():
    print("ğŸ’» REALISTICKÃ HW ANALÃZA PRO MyCoder")
    print("=" * 60)

    print("ğŸ” TVÅ®J SOUÄŒASNÃ HARDWARE:")
    print("   RAM: Kolik mÃ¡Å¡ aktuÃ¡lnÄ›? (napÅ™. 16GB, 32GB)")
    print("   GPU: Jakou mÃ¡Å¡ grafiku? (napÅ™. GTX 1060, RTX 3060, integrated)")
    print("   CPU: Kolik jader? (napÅ™. 4, 6, 8 cores)")
    print("   Disk: Kolik volnÃ©ho mÃ­sta? (pro modely)")

    print(f"\nğŸ“Š REÃLNÃ‰ POÅ½ADAVKY PRO RÅ®ZNÃ‰ MODELY:")

    # Realistic model requirements with overhead
    models = {
        "tinyllama": {
            "model_size": "637 MB",
            "ram_needed": "2-3 GB",  # Model + system + overhead
            "disk_space": "1 GB",
            "description": "Funguje na Äemkoliv",
            "quality": "â­â­â˜†â˜†â˜†",
            "realistic": "âœ… Funguje i na 4GB RAM PC"
        },
        "deepseek-coder-1.3b": {
            "model_size": "750 MB",
            "ram_needed": "3-4 GB",
            "disk_space": "1 GB",
            "description": "MalÃ½ ale pouÅ¾itelnÃ½",
            "quality": "â­â­â­â˜†â˜†",
            "realistic": "âœ… Funguje na 8GB RAM PC"
        },
        "llama3.2-3b": {
            "model_size": "2 GB",
            "ram_needed": "6-8 GB",
            "disk_space": "3 GB",
            "description": "DobrÃ½ kompromis",
            "quality": "â­â­â­â­â˜†",
            "realistic": "âœ… Funguje na 16GB RAM PC"
        },
        "codellama-7b": {
            "model_size": "3.8 GB",
            "ram_needed": "8-12 GB",
            "disk_space": "5 GB",
            "description": "SolidnÃ­ kÃ³dovÃ¡nÃ­",
            "quality": "â­â­â­â­â­",
            "realistic": "âš ï¸  PotÅ™ebuje min. 16GB RAM"
        },
        "codestral-22b": {
            "model_size": "13 GB",
            "ram_needed": "24-32 GB",  # Realisticky!
            "disk_space": "15 GB",
            "description": "Premium, ale Å¾ere RAM",
            "quality": "â­â­â­â­â­",
            "realistic": "âŒ PotÅ™ebuje 32GB+ RAM reÃ¡lnÄ›"
        }
    }

    print("   Model              Velikost  RAM potÅ™eba  Disk  Kvalita     Realita")
    print("   " + "="*80)

    for model, specs in models.items():
        print(f"   {model:<17} {specs['model_size']:<9} {specs['ram_needed']:<11} {specs['disk_space']:<5} {specs['quality']:<11} {specs['realistic']}")
        print(f"   {'':>18} â†’ {specs['description']}")
        print()

    print(f"ğŸ’¡ PROÄŒ CODESTRAL POTÅ˜EBUJE TOLIK RAM:")
    print("   ğŸ”¹ Model: 13GB")
    print("   ğŸ”¹ Ollama overhead: +2-3GB")
    print("   ğŸ”¹ Docker overhead: +2GB")
    print("   ğŸ”¹ System rezerva: +4-6GB")
    print("   ğŸ”¹ Inference buffer: +3-4GB")
    print("   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("   ğŸ“Š CELKEM: 24-28GB RAM reÃ¡lnÄ›!")

    print(f"\nğŸ–¥ï¸  TVOJE MOÅ½NOSTI PODLE HW:")

    hw_scenarios = {
        "StarÅ¡Ã­ PC (8GB RAM)": {
            "recommended": "deepseek-coder-1.3b",
            "max_model": "llama3.2-1b",
            "docker_size": "1.5 GB",
            "performance": "PomalÃ©, ale funkÄnÃ­",
            "coding_quality": "ZÃ¡kladnÃ­ kÃ³dovÃ¡nÃ­ OK"
        },
        "BÄ›Å¾nÃ½ PC (16GB RAM)": {
            "recommended": "llama3.2-3b + deepseek",
            "max_model": "codellama-7b (s omezenÃ­m)",
            "docker_size": "4-5 GB",
            "performance": "DobrÃ¡ rychlost",
            "coding_quality": "VÃ½bornÃ© pro bÄ›Å¾nÃ½ vÃ½voj"
        },
        "Gaming PC (32GB RAM)": {
            "recommended": "codellama-7b + llama3.2-3b",
            "max_model": "codestral-22b (tÄ›snÄ›)",
            "docker_size": "18 GB",
            "performance": "Velmi dobrÃ¡",
            "coding_quality": "ProfesionÃ¡lnÃ­ ÃºroveÅˆ"
        },
        "Server/Workstation (64GB+)": {
            "recommended": "codestral-22b + vÃ­ce modelÅ¯",
            "max_model": "VÅ¡echny modely",
            "docker_size": "35+ GB",
            "performance": "ExcelentnÃ­",
            "coding_quality": "Maximum moÅ¾nÃ©"
        }
    }

    print("   HW Konfigurace         DoporuÄenÃ½ model        Docker  VÃ½kon")
    print("   " + "="*75)

    for hw, specs in hw_scenarios.items():
        print(f"   {hw:<22} {specs['recommended']:<23} {specs['docker_size']:<7} {specs['performance']}")
        print(f"   {'':>23} Max: {specs['max_model']}")
        print(f"   {'':>23} â†’ {specs['coding_quality']}")
        print()

    print(f"âš¡ PRAKTICKÃ‰ DOPORUÄŒENÃ PRO TVÅ®J SERVER:")
    print("   ğŸ¯ Pokud mÃ¡Å¡ 16GB RAM â†’ Balanced scÃ©nÃ¡Å™ (codellama-7b)")
    print("   ğŸ¯ Pokud mÃ¡Å¡ 32GB RAM â†’ Premium moÅ¾nÃ©, ale tÄ›snÄ›")
    print("   ğŸ¯ Pokud mÃ¡Å¡ 64GB+ RAM â†’ Codestral bez problÃ©mÅ¯")
    print()
    print("   ğŸ’° CENOVÄš EFEKTIVNÃ:")
    print("   âœ… llama3.2-3b (2GB) - velmi dobrÃ½ pomÄ›r kvality/HW")
    print("   âœ… deepseek-coder-1.3b (750MB) - specializovanÃ½ na kÃ³d")
    print("   âœ… codellama-7b (3.8GB) - pokud mÃ¡Å¡ dost RAM")
    print()
    print("   âŒ POZOR - CODESTRAL:")
    print("   ğŸ“Š 13GB model + overhead = reÃ¡lnÄ› 24-28GB RAM")
    print("   ğŸ’¸ DrahÃ© na HW poÅ¾adavky")
    print("   ğŸŒ PomalÃ© na slabÅ¡Ã­m HW")

    print(f"\nğŸš€ MOJE KONKRÃ‰TNÃ DOPORUÄŒENÃ:")
    print("   1ï¸âƒ£  START: deepseek-coder-1.3b (750MB) - funguje vÅ¡ude")
    print("   2ï¸âƒ£  UPGRADE: llama3.2-3b (2GB) - skvÄ›lÃ½ pomÄ›r")
    print("   3ï¸âƒ£  PRO: codellama-7b (3.8GB) - pokud mÃ¡Å¡ 16GB+ RAM")
    print("   4ï¸âƒ£  PREMIUM: codestral-22b (13GB) - jen s 32GB+ RAM")

    print(f"\nğŸ’¡ PRO TVÅ®J SERVER - REALISTICKÃ‰ PLÃNY:")
    server_plans = [
        "ğŸ“¦ Minimum viable: deepseek + tinyllama (1.4GB, 8GB RAM)",
        "ğŸ¯ DoporuÄeno: llama3.2-3b + deepseek (2.8GB, 16GB RAM)",
        "ğŸš€ Premium: codellama-7b + llama3.2 (5.8GB, 24GB RAM)",
        "ğŸ‘‘ Ultimate: codestral jen pokud mÃ¡Å¡ 32GB+ RAM"
    ]

    for plan in server_plans:
        print(f"   {plan}")

if __name__ == "__main__":
    realistic_hw_analysis()