#!/usr/bin/env python3
"""
KalkulÃ¡tor velikostÃ­ MyCoder Docker instalace s Ollama modely
"""

def show_docker_sizes():
    print("ğŸ³ MyCoder Docker - VELIKOSTI INSTALACE")
    print("=" * 60)
    
    # Base components
    components = {
        "Base Ubuntu/Python image": "~400 MB",
        "MyCoder zdrojovÃ½ kÃ³d": "~5 MB", 
        "Python zÃ¡vislosti": "~30 MB",
        "Ollama binary": "~150 MB",
    }
    
    print("ğŸ—ï¸  BASE KOMPONENTY:")
    total_base = 585  # MB
    for component, size in components.items():
        print(f"   ğŸ“¦ {component:<30} {size:>12}")
    print(f"   {'='*44}")
    print(f"   ğŸ“Š Base celkem: {total_base:>25} MB")
    
    # Ollama models
    print(f"\nğŸ¤– OLLAMA MODELY:")
    models = {
        # Mistral AI code models (TOP TIER)
        "codestral:22b-v0.1-q4_0": "13.0 GB",      # Best for coding
        "codestral:22b-v0.1-q8_0": "23.0 GB",      # Highest quality
        "mistral:7b-instruct-v0.3-q4_0": "4.1 GB", # General Mistral
        
        # Meta code models  
        "codellama:7b-instruct": "3.8 GB",
        "codellama:7b-instruct-q4_0": "2.0 GB", 
        
        # Specialized code models
        "deepseek-coder:1.3b-base-q4_0": "750 MB",
        "deepseek-coder:6.7b-instruct-q4_0": "3.8 GB",
        
        # General models  
        "llama3.2:3b-instruct-q4_0": "2.0 GB",
        "llama3.2:1b-instruct-q4_0": "1.3 GB",
        
        # Tiny models for testing
        "tinyllama:1.1b-chat-v1.0": "637 MB",
    }
    
    print("   Model                           Velikost    PouÅ¾itÃ­")
    print("   " + "="*55)
    
    model_sizes_gb = {}
    for model, size in models.items():
        use_case = {
            "codestral:22b-v0.1-q4_0": "ğŸ‘‘ NejlepÅ¡Ã­ kÃ³d",
            "codestral:22b-v0.1-q8_0": "ğŸ’ Premium kÃ³d",
            "mistral:7b-instruct-v0.3-q4_0": "ğŸ¯ Mistral AI",
            "codellama:7b-instruct": "ğŸ¦™ Meta kÃ³d", 
            "codellama:7b-instruct-q4_0": "âš¡ RychlÃ½ kÃ³d",
            "deepseek-coder:1.3b-base-q4_0": "ğŸ”§ MalÃ½ kÃ³d",
            "deepseek-coder:6.7b-instruct-q4_0": "ğŸ§  PokroÄilÃ½ kÃ³d",
            "llama3.2:3b-instruct-q4_0": "ğŸ’¬ General AI",
            "llama3.2:1b-instruct-q4_0": "âš¡ RychlÃ½ AI",
            "tinyllama:1.1b-chat-v1.0": "ğŸ§ª Testing"
        }.get(model, "ğŸ“‹ General")
        
        print(f"   {model:<30} {size:>8} {use_case}")
        
        # Convert to MB for calculations
        if "GB" in size:
            model_sizes_gb[model] = float(size.split()[0]) * 1000
        else:
            model_sizes_gb[model] = float(size.split()[0])
    
    # Configuration scenarios  
    print(f"\nğŸ¯ INSTALAÄŒNÃ SCÃ‰NÃÅ˜E:")
    scenarios = {
        "Minimum (testing)": {
            "models": ["tinyllama:1.1b-chat-v1.0"],
            "use_case": "Test & development"
        },
        "Lightweight": {
            "models": ["deepseek-coder:1.3b-base-q4_0", "llama3.2:1b-instruct-q4_0"],
            "use_case": "Fast coding assistant"
        },
        "Balanced": {
            "models": ["codellama:7b-instruct-q4_0", "llama3.2:3b-instruct-q4_0"],
            "use_case": "Good performance/size ratio"
        },
        "Premium": {
            "models": ["codestral:22b-v0.1-q4_0", "mistral:7b-instruct-v0.3-q4_0"],
            "use_case": "Best Mistral AI coding"
        },
        "Ultimate": {
            "models": ["codestral:22b-v0.1-q8_0", "codestral:22b-v0.1-q4_0"],
            "use_case": "Maximum code quality"
        },
        "Complete": {
            "models": list(models.keys()),
            "use_case": "All models available"
        }
    }
    
    print("   ScÃ©nÃ¡Å™        Velikost     Modely                    PouÅ¾itÃ­")
    print("   " + "="*75)
    
    for scenario, config in scenarios.items():
        total_size = total_base
        for model in config["models"]:
            total_size += model_sizes_gb[model]
        
        size_str = f"{total_size/1000:.1f} GB" if total_size > 1000 else f"{total_size:.0f} MB"
        model_count = len(config["models"])
        
        print(f"   {scenario:<12} {size_str:>8} {model_count:>2} models            {config['use_case']}")
    
    print(f"\nğŸ’¾ DISKOVÃ PROSTOR:")
    print("   ğŸ”¹ DoporuÄeno: min. 10 GB volnÃ©ho mÃ­sta") 
    print("   ğŸ”¹ OptimÃ¡lnÃ­: 15-20 GB pro vÅ¡echny modely")
    print("   ğŸ”¹ Docker volumes: dalÅ¡Ã­ 2-5 GB")
    
    print(f"\nğŸš€ DOPORUÄŒENÃ PRO RÅ®ZNÃ‰ POUÅ½ITÃ:")
    recommendations = {
        "VÃ½vojÃ¡Å™ (laptop)": "Lightweight - 2.6 GB",
        "KÃ³dovÃ¡nÃ­ (desktop)": "Balanced - 4.6 GB", 
        "Mistral fan": "Premium - 17.7 GB",
        "AI research": "Ultimate - 36.6 GB",
        "Production server": "Complete - vÅ¡echny modely"
    }
    
    for use_case, recommendation in recommendations.items():
        print(f"   ğŸ‘¤ {use_case:<20} â†’ {recommendation}")
    
    print(f"\nâš¡ VÃKON vs VELIKOST:")
    print("   ğŸ“Š tinyllama (637 MB)     - â­â­â˜†â˜†â˜† kvalita, â­â­â­â­â­ rychlost")
    print("   ğŸ“Š deepseek-1.3b (750 MB) - â­â­â­â˜†â˜† kvalita, â­â­â­â­â˜† rychlost") 
    print("   ğŸ“Š llama3.2-3b (2 GB)     - â­â­â­â­â˜† kvalita, â­â­â­â˜†â˜† rychlost")
    print("   ğŸ“Š codellama-7b (3.8 GB)  - â­â­â­â­â­ kvalita, â­â­â˜†â˜†â˜† rychlost")
    print("   ğŸ“Š codestral-22b (13 GB)  - â­â­â­â­â­ kvalita, â­â­â˜†â˜†â˜† rychlost")
    print("   ğŸ‘‘ CODESTRAL = NEJLEPÅ Ã PRO KÃ“DOVÃNÃ!")
    
    print(f"\nğŸ† CODESTRAL BENEFITS:")
    print("   âœ… SpecializovanÃ½ na programovÃ¡nÃ­")
    print("   âœ… Podporuje 80+ prog. jazykÅ¯")
    print("   âœ… VÃ½bornÃ¡ kontextovÃ¡ pamÄ›Å¥")  
    print("   âœ… PÅ™esnÃ© type hints a dokumentace")
    print("   âœ… PokroÄilÃ© debugging nÃ¡vrhy")

if __name__ == "__main__":
    show_docker_sizes()