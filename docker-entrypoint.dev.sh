#!/bin/bash
set -e

echo "ğŸš€ MyCoder Development Container Starting"
echo "========================================"
echo "ğŸ”¥ LIVE RELOAD MODE - zmÄ›ny se projevÃ­ okamÅ¾itÄ›!"
echo ""

# Development environment info
echo "ğŸ“Š Development Environment:"
echo "   ğŸ Python: $(python --version)"
echo "   ğŸ“¦ Poetry: $(poetry --version 2>/dev/null || echo 'Not available')"
echo "   ğŸ¤– Ollama: $(ollama --version 2>/dev/null || echo 'Starting...')"
echo "   ğŸ“‚ Working Dir: $(pwd)"
echo "   ğŸ‘¤ User: $(whoami)"
echo ""

# Start Ollama service in background
echo "ğŸš€ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!
echo "   Ollama PID: $OLLAMA_PID"

# Wait for Ollama to be ready (faster timeout for dev)
echo "â³ Waiting for Ollama to be ready..."
for i in {1..15}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama is ready!"
        break
    fi
    echo "   Attempt $i/15..."
    sleep 1
done

# Check if Ollama started successfully
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âŒ Ollama failed to start within timeout"
    echo "ğŸ”§ Trying to start Ollama manually..."
    pkill ollama 2>/dev/null || true
    sleep 2
    ollama serve &
    OLLAMA_PID=$!
    sleep 5
fi

# Pull DeepSeek model (with progress for dev)
echo "ğŸ” Checking for DeepSeek model..."
if ! ollama list | grep -q "deepseek-coder"; then
    echo "ğŸ“¥ Pulling DeepSeek Coder model for development..."
    echo "   This is one-time setup - subsequent starts will be fast!"
    ollama pull deepseek-coder:1.3b-base-q4_0 && echo "âœ… DeepSeek model ready!"
else
    echo "âœ… DeepSeek model already available"
fi

# Show available models
echo ""
echo "ğŸ¤– Available models:"
ollama list | tail -n +2 | while read line; do
    echo "   â€¢ $line"
done

# Development-specific setup
echo ""
echo "ğŸ”§ Development Setup:"

# Check if source code is mounted
if [ -d "/app/src" ] && [ "$(ls -A /app/src 2>/dev/null)" ]; then
    echo "   âœ… Source code mounted (live reload active)"
    ls -la /app/src | head -5
else
    echo "   âš ï¸  Source code not mounted - live reload disabled"
fi

# Check for .env.dev
if [ -f "/app/.env" ]; then
    echo "   âœ… Development environment loaded"
    echo "   ğŸ“‹ Key settings:"
    grep -E "MYCODER_|OLLAMA_|LOG_LEVEL" /app/.env | head -5 | sed 's/^/      /'
else
    echo "   âš ï¸  No .env file found"
fi

# Python path info  
echo "   ğŸ Python path: $PYTHONPATH"
echo "   ğŸ” Can import mycoder: $(python -c 'import src.mycoder; print("âœ… YES")' 2>/dev/null || echo 'âŒ NO')"

# Start development server
echo ""
echo "ğŸŒŸ MyCoder Development Server Starting!"
echo "========================================"
echo "ğŸ”— APIs available:"
echo "   â€¢ Ollama: http://localhost:11434"
echo "   â€¢ MyCoder: http://localhost:8000"  
echo "   â€¢ Debugger: localhost:5678"
echo ""
echo "ğŸš€ Development Commands:"
echo "   docker-compose -f docker-compose.dev.yml exec mycoder-dev bash"
echo "   docker-compose -f docker-compose.dev.yml logs -f"
echo "   make dev-shell  # if using Makefile"
echo ""

# Check if we should start in interactive mode or service mode
if [ "$1" = "interactive" ] || [ "$MYCODER_DEV_MODE" = "interactive" ]; then
    echo "ğŸ Starting interactive Python session..."
    python -c "
import sys
sys.path.insert(0, '/app/src')
print('ğŸ¯ MyCoder Development Session')
print('Available modules:')
try:
    from src.mycoder import MyCoder
    print('  â€¢ MyCoder âœ…')
except ImportError as e:
    print(f'  â€¢ MyCoder âŒ ({e})')

try:
    from src.ollama_integration import OllamaClient, CodeGenerationProvider
    print('  â€¢ Ollama Integration âœ…')
except ImportError as e:
    print(f'  â€¢ Ollama Integration âŒ ({e})')

print('\\nQuick test:')
import asyncio

async def test():
    from src.ollama_integration import OllamaClient
    async with OllamaClient() as client:
        available = await client.is_available()
        print(f'Ollama connection: {\"âœ… OK\" if available else \"âŒ FAILED\"}')
        
        if available:
            models = await client.list_models()
            print(f'Models available: {len(models)}')

try:
    asyncio.run(test())
except Exception as e:
    print(f'Test failed: {e}')

print('\\nğŸš€ Ready for development!')
print('Pro debugging: import ipdb; ipdb.set_trace()')
"
    
    # Keep container running  
    echo ""
    echo "ğŸ’¤ Container running in development mode..."
    echo "   Press Ctrl+C to stop"
    trap 'echo "ğŸ›‘ Stopping development server..."; kill $OLLAMA_PID 2>/dev/null; exit 0' INT TERM
    
    # Optional: Start file watcher
    if [ "$WATCHDOG_ENABLED" = "true" ]; then
        echo "ğŸ‘€ Starting file watcher..."
        python -c "
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import subprocess
import os

class CodeChangeHandler(FileSystemEventHandler):
    def on_modified(self, event):
        if event.is_directory:
            return
        if event.src_path.endswith(('.py', '.yml', '.yaml', '.json')):
            print(f'ğŸ”„ File changed: {event.src_path}')
            print('   Code is live-reloaded automatically!')

observer = Observer()
if os.path.exists('/app/src'):
    observer.schedule(CodeChangeHandler(), '/app/src', recursive=True)
    print('ğŸ“‚ Watching /app/src for changes...')
    observer.start()

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    observer.stop()
    print('ğŸ‘€ File watcher stopped')
observer.join()
" &
    fi
    
    wait

elif [ "$1" = "test" ]; then
    echo "ğŸ§ª Running development tests..."
    if [ -d "/app/tests" ]; then
        python -m pytest /app/tests -v --tb=short
    else
        echo "âš ï¸  No tests directory found"
        python -c "
import sys
sys.path.insert(0, '/app/src')
from src.ollama_integration import OllamaClient
import asyncio

async def quick_test():
    print('ğŸ” Quick integration test...')
    async with OllamaClient() as client:
        if await client.is_available():
            print('âœ… Ollama connection OK')
            models = await client.list_models()
            print(f'âœ… Found {len(models)} models')
            return True
        else:
            print('âŒ Ollama connection failed')
            return False

result = asyncio.run(quick_test())
exit(0 if result else 1)
"
    fi

elif [ "$1" = "demo" ]; then
    echo "ğŸ¬ Running MyCoder demo..."
    python -c "
import sys
sys.path.insert(0, '/app/src')
print('ğŸ¯ MyCoder Development Demo')
# Add demo code here
"

else
    # Default: run service mode
    echo "ğŸŒ Starting MyCoder service (development mode)..."
    
    # Keep container alive and responsive
    trap 'echo "ğŸ›‘ Stopping services..."; kill $OLLAMA_PID 2>/dev/null; exit 0' INT TERM
    
    # Start MyCoder in background
    python -c "
import sys
sys.path.insert(0, '/app/src')
import asyncio
import signal

async def run_mycoder():
    try:
        from src.mycoder import MyCoder
        print('ğŸ¯ Starting MyCoder service...')
        mycoder = MyCoder()
        print(f'âœ… MyCoder ready in mode: {getattr(mycoder.mode_manager.current_mode, \"value\", \"unknown\")}')
        
        # Keep service running
        while True:
            await asyncio.sleep(1)
            
    except KeyboardInterrupt:
        print('ğŸ›‘ MyCoder service stopped')
    except Exception as e:
        print(f'âŒ MyCoder service error: {e}')
        import traceback
        traceback.print_exc()

asyncio.run(run_mycoder())
" &
    
    echo "ğŸ’¤ Services running... Press Ctrl+C to stop"
    wait
fi