#!/bin/bash
set -e

echo "ðŸ¤– MyCoder - Starting Docker Container"
echo "======================================"

# Start Ollama service in background
echo "ðŸš€ Starting Ollama service..."
ollama serve &
OLLAMA_PID=$!

# Wait for Ollama to be ready
echo "â³ Waiting for Ollama to be ready..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama is ready!"
        break
    fi
    echo "   Attempt $i/30..."
    sleep 2
done

# Pull DeepSeek Coder model first (optimized for Q9550 hardware)
echo "ðŸ” Checking for DeepSeek model..."
if ! ollama list | grep -q "deepseek-coder"; then
    echo "ðŸ“¥ Pulling DeepSeek Coder model (optimized, lightweight - 750MB)..."
    ollama pull deepseek-coder:1.3b-base-q4_0
    echo "âœ… DeepSeek Coder model ready!"
else
    echo "âœ… DeepSeek Coder model already available"
fi

# Fallback: Pull CodeLlama if DeepSeek fails
if ! ollama list | grep -q "deepseek-coder\|codellama"; then
    echo "ðŸ“¥ Pulling CodeLlama as fallback..."
    ollama pull codellama:7b-instruct-q4_0
fi

# Optional: Pull other useful models (in fallback order)
echo "ðŸ” Setting up additional models..."
models_to_pull=(
    "llama3.2:3b-instruct-q4_0"     # Fast, small model
    "codellama:7b-instruct-q4_0"    # Meta's code model (fallback)
)

for model in "${models_to_pull[@]}"; do
    if ! ollama list | grep -q "${model%%:*}"; then
        echo "ðŸ“¥ Pulling $model..."
        ollama pull "$model" || echo "âš ï¸  Failed to pull $model"
    fi
done

# Start MyCoder
echo "ðŸŽ¯ Starting MyCoder..."
echo "Available models:"
ollama list

# Keep container running and show logs
echo "ðŸŒŸ MyCoder Docker container is ready!"
echo "======================================"
echo "ðŸ”— Ollama API: http://localhost:11434"
echo "ðŸ¤– Available models:"
ollama list | tail -n +2
echo "======================================"

# Start interactive Python session or run specific command
if [ "$1" = "interactive" ]; then
    echo "ðŸ Starting interactive Python session..."
    python -c "
from mycoder import MyCoder
import asyncio

async def main():
    mycoder = MyCoder()
    print('ðŸŽ¯ MyCoder ready with Ollama integration!')
    print(f'Current mode: {mycoder.mode_manager.current_mode.value}')
    
    # Test Ollama connection
    result = await mycoder.process_request('Hello from Docker! Create a simple Python function.')
    print('Response:', result.get('content', 'No response'))

if __name__ == '__main__':
    asyncio.run(main())
"
elif [ "$1" = "test" ]; then
    echo "ðŸ§ª Running MyCoder tests..."
    python -m pytest tests/ -v || echo "âš ï¸  Some tests may require external services"
elif [ "$1" = "demo" ]; then
    echo "ðŸŽ¬ Running MyCoder demo..."
    python quick_start.py
else
    # Keep container alive
    echo "ðŸ’¤ Container running... Press Ctrl+C to stop"
    trap 'echo "ðŸ›‘ Stopping services..."; kill $OLLAMA_PID 2>/dev/null; exit 0' INT TERM
    wait
fi