#!/bin/bash
set -e

echo "ðŸª¶ MyCoder Ultra-Lightweight Container Starting"
echo "=============================================="
echo "ðŸ’¡ Optimalized pro slabÅ¡Ã­ hardware!"
echo "ðŸ“Š MinimÃ¡lnÃ­ spotÅ™eba: 2-4GB RAM, 1-2 CPU cores"
echo ""

# Lightweight environment info
echo "ðŸ“Š Lightweight Environment:"
echo "   ðŸ Python: $(python --version)"
echo "   ðŸ¤– Ollama: $(ollama --version 2>/dev/null || echo 'Starting...')"
echo "   ðŸ“‚ Working Dir: $(pwd)"
echo "   ðŸ’¾ Available RAM: $(free -h | grep ^Mem | awk '{print $7}')"
echo ""

# Start Ollama s resource constraints
echo "ðŸš€ Starting Ollama (lightweight mode)..."
export OLLAMA_MAX_LOADED_MODELS=1
export OLLAMA_NUM_PARALLEL=1
export OLLAMA_MAX_VRAM=0  # CPU-only
ollama serve &
OLLAMA_PID=$!
echo "   Ollama PID: $OLLAMA_PID (resource-limited)"

# KratÅ¡Ã­ timeout pro lightweight
echo "â³ Waiting for Ollama (lightweight timeout)..."
for i in {1..10}; do
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "âœ… Ollama ready!"
        break
    fi
    echo "   Attempt $i/10..."
    sleep 2
done

# Check Ollama startup
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "âŒ Ollama failed to start"
    exit 1
fi

# Ensure TinyLlama is available (uÅ¾ by mÄ›l bÃ½t staÅ¾enÃ½)
echo "ðŸ” Checking TinyLlama model..."
if ! ollama list | grep -q "tinyllama"; then
    echo "ðŸ“¥ Pulling TinyLlama (lightweight - 637MB)..."
    ollama pull tinyllama:1.1b
    echo "âœ… TinyLlama ready!"
else
    echo "âœ… TinyLlama already available"
fi

# Show available models (should be minimal)
echo ""
echo "ðŸ¤– Available models (lightweight setup):"
ollama list | tail -n +2 | while read line; do
    echo "   â€¢ $line"
done

# Memory usage info
echo ""
echo "ðŸ“Š Current Resource Usage:"
echo "   ðŸ’¾ RAM: $(free -h | grep ^Mem | awk '{print $3 "/" $2}')"
echo "   ðŸ–¥ï¸  CPU: $(nproc) cores available"
echo "   ðŸ“¦ Models loaded: 1 (TinyLlama)"

# Lightweight test
echo ""
echo "ðŸ§ª Lightweight functionality test..."
if ollama show tinyllama:1.1b >/dev/null 2>&1; then
    echo "   âœ… TinyLlama model accessible"
    
    # Quick generation test (timeout rychle)
    echo "   ðŸŽ¯ Testing AI generation..."
    timeout 15s ollama run tinyllama:1.1b "def hello():" > /tmp/test_output 2>&1 && \
    echo "   âœ… AI generation working" || \
    echo "   âš ï¸  AI generation slow (expected on weak hardware)"
else
    echo "   âŒ TinyLlama model issue"
fi

# Start lightweight MyCoder service
echo ""
echo "ðŸŒŸ MyCoder Lightweight Service Starting!"
echo "========================================"
echo "ðŸ”— APIs:"
echo "   â€¢ Ollama: http://localhost:11434"
echo "   â€¢ MyCoder: http://localhost:8000"
echo ""
echo "ðŸª¶ Lightweight Mode Features:"
echo "   â€¢ TinyLlama AI model (637MB)"
echo "   â€¢ CPU-only inference"
echo "   â€¢ Single model loading"
echo "   â€¢ Reduced memory footprint"
echo ""

# Simple service mode (ne interactive)
echo "ðŸ’¤ Lightweight service running..."
echo "   Resource usage minimized"
echo "   Press Ctrl+C to stop"

# Trap pro cleanup
trap 'echo "ðŸ›‘ Stopping lightweight services..."; kill $OLLAMA_PID 2>/dev/null; exit 0' INT TERM

# Keep lightweight service alive
if [ "$1" = "lightweight" ] || [ -z "$1" ]; then
    # Monitor resource usage periodically
    while true; do
        sleep 30
        # Optional: Log resource usage kaÅ¾dÃ½ch 30 sekund
        if [ "$MYCODER_DEBUG" = "true" ]; then
            echo "[$(date)] RAM: $(free -h | grep ^Mem | awk '{print $3}')"
        fi
    done
else
    echo "ðŸ”§ Custom command: $*"
    exec "$@"
fi